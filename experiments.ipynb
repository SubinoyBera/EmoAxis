{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01d23a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"C:\\\\Users\\\\subin\\\\.cache\\\\huggingface\\\\hub\\\\models--sentence-transformers--all-MiniLM-L6-v2\\\\snapshots\\\\all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3db031b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\My Projects\\Tiny Llama\\finetune.env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61c79683",
   "metadata": {},
   "outputs": [],
   "source": [
    "statement1 = model.encode('joy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fdd300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "statement2 = model.encode('sadness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "419685a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = util.cos_sim(statement1, statement2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0d2d951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4299]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f445845",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\My Projects\\CAPSTONE RESEARCH\\finetune.env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_path = \"C:\\\\Users\\\\subin\\\\.cache\\\\huggingface\\\\hub\\\\models--SamLowe--roberta-base-go_emotions\\\\snapshots\\\\roberta-goemo\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3000e208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6461, -5.8056, -5.7108, -5.4754, -3.9026, -5.8223, -5.8843, -6.3264,\n",
       "         -5.5088, -5.5651, -5.4251, -6.2885, -7.5970, -5.6135, -7.1344, -4.9963,\n",
       "         -7.9100, -4.5098,  2.8664, -7.6593, -5.1790, -7.7158, -5.3941, -7.9463,\n",
       "         -6.6800, -5.6090, -6.3140, -4.6681]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_classification(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        return outputs\n",
    "\n",
    "txt = \"I love the book\"\n",
    "pred = get_classification(txt, model, tokenizer)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008346c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMILARITY TEST\n",
    "\n",
    "import torch\n",
    "from torch.nn.functional import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def get_sentence_embedding(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        last_hidden_state = outputs.hidden_states[-1]\n",
    "\n",
    "        # Mean pooling\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
    "        #sentence_embedding = sum_embeddings / sum_mask\n",
    "\n",
    "    return mask_expanded\n",
    "\n",
    "s1 = \"I love my father\"\n",
    "#e1 = 'love'\n",
    "#\n",
    "#s2 = \"I lost my mom, RIP\"\n",
    "#e2 = 'sadness'\n",
    "\n",
    "\n",
    "s1_emb = get_sentence_embedding(s1, model, tokenizer)\n",
    "#e1_emb = get_sentence_embedding(e1, model, tokenizer)\n",
    "#s2_emb = get_sentence_embedding(s2, model, tokenizer)\n",
    "#e2_emb = get_sentence_embedding(s2, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e1f0fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cbd18a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2503473460674286"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1 = torch.cat((s1_emb, e1_emb), dim=-1)\n",
    "c2 = torch.cat((s2_emb, e2_emb), dim=-1)\n",
    "\n",
    "cosine_similarity(c1, c2).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370801cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92d5965f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMILARITY TEST\n",
    "\n",
    "import torch\n",
    "from torch.nn.functional import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def get_sentence_embedding(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        last_hidden_state = outputs.hidden_states[-1]\n",
    "\n",
    "        # Mean pooling\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
    "        sentence_embedding = sum_embeddings / sum_mask\n",
    "\n",
    "    return sentence_embedding\n",
    "\n",
    "s1 = \"satisfied with one's achievement or high standards\"\n",
    "s2 = \"feeling of worry or distress being lifted\"\n",
    "\n",
    "\n",
    "s1_emb = get_sentence_embedding(s1, model, tokenizer)\n",
    "s2_emb = get_sentence_embedding(s2, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22f16f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26255878806114197"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(s1_emb, s2_emb).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9202e906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22648681700229645"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.2658894658088684"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50601cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.11941459774971008"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60583991",
   "metadata": {},
   "source": [
    "### CLS Embedding Fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a47c858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLS TESTING\n",
    "\n",
    "text1 = \"I lost my mom, RIP\"\n",
    "\n",
    "inputs = tokenizer(text1, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "last_hidden_state = outputs.hidden_states[-1]\n",
    "\n",
    "cls_embedding_text1 = last_hidden_state[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "397e52d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_embedding_text1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1f60bc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "E1 = cls_embedding_text1*cls_embedding_emotion1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e772d894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9417742490768433"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "cosine_similarity(cls_embedding_text1, cls_embedding_emotion1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "36ca27cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLS TESTING\n",
    "\n",
    "text2 = \"I love my father\"\n",
    "\n",
    "inputs = tokenizer(text1, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "last_hidden_state = outputs.hidden_states[-1]\n",
    "\n",
    "cls_embedding_text2 = last_hidden_state[:, 0, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9fc8bc65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999403953552"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(cls_embedding_text1, cls_embedding_text2).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6bfc109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "E2 = cls_embedding_text2*cls_embedding_emotion2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "148c6b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.378205269575119"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(E1, E5).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ef19be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3de697fb",
   "metadata": {},
   "source": [
    "# RoBERTa-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1879b530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\My Projects\\CAPSTONE RESEARCH\\finetune.env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23e9e06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\My Projects\\CAPSTONE RESEARCH\\finetune.env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\subin\\.cache\\huggingface\\hub\\models--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = AutoModel.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82c75c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4fa8ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
