{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2f5dd894",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f5dd894",
        "outputId": "702ccd19-aabd-4de5-e7c3-750b346af48a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# DOWNLOAD PRETRAINED MODEL AND TOKENIZER\n",
        "roberta_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "roberta_base = AutoModel.from_pretrained(\"roberta-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6abddf4c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6abddf4c",
        "outputId": "122f17f5-fd2d-495d-96e4-9b2821cfa5a8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "roberta_base.config.hidden_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "151acca4",
      "metadata": {
        "id": "151acca4"
      },
      "outputs": [],
      "source": [
        "# Remove the pooler layer\n",
        "roberta_base.pooler = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3215fcf1",
      "metadata": {
        "id": "3215fcf1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "train_path = \"/content/emopillar_train_filtered.csv\"       # enter path to train dataset\n",
        "val_path = \"/content/emopillar_validation_filtered.csv\"         # enter path to validation dataset\n",
        "test_path = \"/content/emopillar_test_filtered.csv\"        # enter path to test dataset\n",
        "\n",
        "\n",
        "df_train = pd.read_csv(train_path)\n",
        "df_val = pd.read_csv(val_path)\n",
        "df_test = pd.read_csv(test_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f39e8822",
      "metadata": {
        "id": "f39e8822"
      },
      "outputs": [],
      "source": [
        "class EmoPillars_Dataset(Dataset):\n",
        "    def __init__(self, data: pd.DataFrame, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = data\n",
        "        self.max_len = 64\n",
        "        self.target_cols = [str(i) for i in range(28)]\n",
        "        self.soft_target_cols = [str(f\"{i}_exp\") for i in range(28)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return(len(self.data))\n",
        "\n",
        "    def __getitem__(self, idx):   #__get_item__  --> __getitem__\n",
        "        item = self.data.iloc[idx]\n",
        "        text = str(item.utterance)\n",
        "        encoding = self.tokenizer.encode_plus(text,\n",
        "                                            add_special_tokens=True,\n",
        "                                            truncation=True,\n",
        "                                            return_tensors='pt',\n",
        "                                            max_length=self.max_len,\n",
        "                                            padding='max_length',\n",
        "                                            return_attention_mask=True)\n",
        "\n",
        "        target = torch.tensor(item[self.target_cols].values.astype('float32'))\n",
        "        soft_target = torch.tensor(item[self.soft_target_cols].values.astype('float32'))\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
        "            \"atten_masks\": encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"hard_target\": target,\n",
        "            \"soft_target\": soft_target\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "30f43768",
      "metadata": {
        "id": "30f43768"
      },
      "outputs": [],
      "source": [
        "data = EmoPillars_Dataset(df_train, roberta_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1e05c804",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e05c804",
        "outputId": "39bca2f7-d93c-43d4-a1da-245289dc1dbd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "82750"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a3a4f9a2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3a4f9a2",
        "outputId": "08a94bdc-cc80-4921-a582-85d594593fe7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([    0,   100,   240,     7,   465,  7330,  2413, 10913,     8,   146,\n",
              "           123,   582,    13,    39, 45492,   219,     4,     2,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1]),\n",
              " 'atten_masks': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'label_names': \"['desire', 'anger']\",\n",
              " 'expressiveness': '[1.0, 0.9]',\n",
              " 'hard_target': tensor([0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " 'soft_target': tensor([0.0000, 0.0000, 0.9000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000])}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.__getitem__(1)   #__get_item__  --> __getitem__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "bf341534",
      "metadata": {
        "id": "bf341534"
      },
      "outputs": [],
      "source": [
        "# Data Loaders\n",
        "train_dataloader = DataLoader(EmoPillars_Dataset(df_train, roberta_tokenizer), batch_size=64, num_workers=4)\n",
        "val_dataloader = DataLoader(EmoPillars_Dataset(df_val, roberta_tokenizer), batch_size=64, num_workers=4)\n",
        "test_dataloader = DataLoader(EmoPillars_Dataset(df_test, roberta_tokenizer), batch_size=64, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da243e7e",
      "metadata": {
        "id": "da243e7e"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "\n",
        "with open(\"label_embeddings.json\", \"r\") as f:\n",
        "    label_embeddings = json.load(f)\n",
        "\n",
        "emo_emb=[]\n",
        "for k, v in label_embeddings.items():\n",
        "    emb = torch.tensor(v, dtype=torch.float32)\n",
        "    emo_emb.append(emb)\n",
        "\n",
        "emotion_bank = torch.cat(emo_emb, dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38b87d7b",
      "metadata": {
        "id": "38b87d7b"
      },
      "source": [
        "## Model Architecture Design"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b7a7075",
      "metadata": {
        "id": "8b7a7075"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from torch.optim import AdamW\n",
        "from torch.amp.autocast_mode import autocast\n",
        "from torch.amp.grad_scaler import GradScaler\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "363b53f0",
      "metadata": {
        "id": "363b53f0"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Text Encoder:\n",
        "    - Takes in tokenized text (from tokenizer)\n",
        "    - Generates the text embedding vector\n",
        "    \"\"\"\n",
        "    def __init__(self, base_encoder):\n",
        "        super().__init__()\n",
        "        self.encoder = base_encoder\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        inputs: tokenizer output dict (input_ids, attention_mask)\n",
        "        \"\"\"\n",
        "        outputs = self.encoder(**inputs, output_hidden_states=True)\n",
        "        last_hidden_state = outputs.hidden_states[-1]                                                                   # [B, T, H]\n",
        "\n",
        "        atten_mask = inputs['attention_mask']                                                                           # [B, T]\n",
        "        # Mean pooling text_embeddings\n",
        "        atten_mask = atten_mask.unsqueeze(-1).float()\n",
        "        pooled_text_emb = (last_hidden_state * atten_mask).sum(dim=1) / atten_mask.sum(dim=1).clamp(min=1e-9)           # [B, H]\n",
        "\n",
        "        return {\n",
        "            \"text_embed\": pooled_text_emb,\n",
        "            \"last_hidden_state\": last_hidden_state,\n",
        "            \"atten_mask\": atten_mask\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3171d01d",
      "metadata": {
        "id": "3171d01d"
      },
      "outputs": [],
      "source": [
        "class CrossAttentionModule(nn.Module):\n",
        "    def __init__(self, hidden_size = 768):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Multi-head cross attention\n",
        "        self.cross_attn = nn.MultiheadAttention(\n",
        "            embed_dim=self.hidden_size,\n",
        "            num_heads=6,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.rms_norm = nn.RMSNorm(hidden_size)\n",
        "\n",
        "    def forward(self, encoder_out, emotion_bank):\n",
        "        # cross-attention (query=text, key/value=emotion)\n",
        "        attn_out, _ = self.cross_attn(\n",
        "            query = encoder_out[\"last_hidden_state\"],                                         # [B, T, 768]\n",
        "            key = emotion_bank.unsqueeze(0).shape,                                            # [1, 28, 768]\n",
        "            value = emotion_bank.unsqueeze(0).shape,                                          # [1, 28, 768]\n",
        "        )\n",
        "\n",
        "        # Fuse and pool\n",
        "        fused_hidden_state = encoder_out[\"last_hidden_state\"] + attn_out\n",
        "        atten_mask = encoder_out[\"atten_mask\"]\n",
        "        fused_emo_text_emb = (fused_hidden_state * atten_mask).sum(dim=1) / atten_mask.sum(dim=1).clamp(min=1e-9)\n",
        "        fused_emo_text_emb = self.rms_norm(fused_emo_text_emb)\n",
        "\n",
        "        return fused_emo_text_emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec475236",
      "metadata": {
        "id": "ec475236"
      },
      "outputs": [],
      "source": [
        "class PosteriorNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Learns an emotion-aware posterior distribution q(z | x, e)\n",
        "    over latent space using fused encoder output.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=768, latent_dim=128):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.GELU(),\n",
        "            nn.RMSNorm(512),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.GELU(),\n",
        "            nn.RMSNorm(256)\n",
        "        )\n",
        "        self.mu_posterior = nn.Linear(256, latent_dim)\n",
        "        self.logvar_posterior = nn.Linear(256, latent_dim)\n",
        "\n",
        "    def forward(self, fused_emo_text_emb):\n",
        "        h = self.mlp(fused_emo_text_emb)\n",
        "        mu_post = self.mu_posterior(h)\n",
        "        logvar_post = torch.clamp(self.logvar_posterior(h), min=-5, max=5)\n",
        "\n",
        "        # Reparameterization trick: sample z ~ N(mu, sigma^2)\n",
        "        std = torch.exp(0.5 * logvar_post)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = mu_post + eps * std\n",
        "\n",
        "        return z, mu_post, logvar_post"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4efaefc2",
      "metadata": {
        "id": "4efaefc2"
      },
      "outputs": [],
      "source": [
        "class PriorNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Learns a prior distribution p(z | x)\n",
        "    based only on text (without emotion labels).\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=768, latent_dim=128):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.GELU(),\n",
        "            nn.RMSNorm(512),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.GELU(),\n",
        "            nn.RMSNorm(256)\n",
        "        )\n",
        "        self.mu_prior = nn.Linear(256, latent_dim)\n",
        "        self.logvar_prior = nn.Linear(256, latent_dim)\n",
        "\n",
        "    def forward(self, text_emb):\n",
        "        h = self.mlp(text_emb)\n",
        "        mu_prior = self.mu_prior(h)\n",
        "        logvar_prior = torch.clamp(self.logvar_prior(h), min=-5, max=5)\n",
        "\n",
        "        return mu_prior, logvar_prior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "8402d8a4",
      "metadata": {
        "id": "8402d8a4"
      },
      "outputs": [],
      "source": [
        "class Classifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Final shared emotion classifier layer\n",
        "    \"\"\"\n",
        "    def __init__(self, latent_dim=128, num_classes=28):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 256),\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(128),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.mlp(z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d61c90f",
      "metadata": {
        "id": "7d61c90f"
      },
      "outputs": [],
      "source": [
        "# Main Model class\n",
        "class EmoAxis(nn.Module):\n",
        "    def __init__(self, encoder, cross_atten_module, posterior_net, prior_net, classifier):\n",
        "        \"\"\"\n",
        "        Architecture:\n",
        "        - EncoderBlock   → produces text and fused(text+emotion) embeddings\n",
        "        - PosteriorNet   → q(z|x,e)\n",
        "        - PriorNet       → p(z|x)\n",
        "        - EmotionClassifier → predicts emotions from latent z\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.cross_atten = cross_atten_module\n",
        "        self.posterior = posterior_net\n",
        "        self.prior = prior_net\n",
        "        self.classifier = classifier\n",
        "\n",
        "\n",
        "    def total_params(self):\n",
        "        \"\"\"Utility function to check trainable vs total params.\"\"\"\n",
        "        total = sum(p.numel() for p in self.parameters())\n",
        "        trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "        print(f\"Total parameters: {total:,}\")\n",
        "        print(f\"Trainable parameters: {trainable:,}\")\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, atten_mask, emotion_bank):\n",
        "        # Encoder\n",
        "        encoder_outputs = self.encoder(\n",
        "            inputs = {\"input_ids\": input_ids, \"attention_mask\": atten_mask},\n",
        "        )\n",
        "        text_emb = encoder_outputs[\"text_embed\"]                  # [B, H]\n",
        "\n",
        "        # Cross Attention\n",
        "        fused_emb = self.cross_atten(encoder_outputs, emotion_bank)    # [B, H]\n",
        "\n",
        "        # Posterior Net\n",
        "        z_post, mu_post, logvar_post = self.posterior(fused_emb)\n",
        "\n",
        "        # Prior Net\n",
        "        mu_prior, logvar_prior = self.prior(text_emb)\n",
        "        z_prior = mu_prior + torch.exp(0.5 * logvar_prior) * torch.randn_like(mu_prior)\n",
        "\n",
        "        # 4. Classifier\n",
        "        logits_post = self.classifier(z_post)           # from sampled posterior\n",
        "        logits_prior = self.classifier(z_prior)         # from prior mean\n",
        "\n",
        "        return {\n",
        "            \"mu_post\": mu_post,\n",
        "            \"logvar_post\": logvar_post,\n",
        "            \"mu_prior\": mu_prior,\n",
        "            \"logvar_prior\": logvar_prior,\n",
        "            \"logits_post\": logits_post,\n",
        "            \"logits_prior\": logits_prior\n",
        "        }\n",
        "\n",
        "\n",
        "    def inference(self, input_ids, atten_mask):\n",
        "        \"\"\"\n",
        "        Predict emotions from raw text (inference using prior p(z|x)).\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            encoder_outputs = self.encoder(\n",
        "                inputs = {\"input_ids\": input_ids, \"attention_mask\": atten_mask},\n",
        "            )\n",
        "            # get only text embeddings\n",
        "            text_emb = encoder_outputs[\"text_embed\"]\n",
        "\n",
        "            # latent mean from prior\n",
        "            mu_prior, _ = self.prior(text_emb)\n",
        "\n",
        "            # classification\n",
        "            logits = self.classifier(mu_prior)\n",
        "\n",
        "            return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c9f26c3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c9f26c3",
        "outputId": "1d104237-0673-46fd-d1b3-e1a3d60b4ad8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total parameters: 127,674,012\n",
            "Trainable parameters: 127,674,012\n"
          ]
        }
      ],
      "source": [
        "encoder = Encoder(base_encoder=roberta_base)\n",
        "\n",
        "cross_atten_module = CrossAttentionModule()\n",
        "posterior_net = PosteriorNetwork()\n",
        "prior_net = PriorNetwork()\n",
        "classifier = Classifier()\n",
        "\n",
        "# Initialize model\n",
        "model = EmoAxis(\n",
        "    encoder=encoder,\n",
        "    cross_atten_module=cross_atten_module,\n",
        "    posterior_net=posterior_net,\n",
        "    prior_net=prior_net,\n",
        "    classifier=classifier\n",
        ")\n",
        "\n",
        "model.total_params()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2db152c",
      "metadata": {
        "id": "d2db152c"
      },
      "source": [
        "## Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69e3c2b9",
      "metadata": {
        "id": "69e3c2b9"
      },
      "outputs": [],
      "source": [
        "def kl_divergence(mu_post, logvar_post, mu_prior, logvar_prior):\n",
        "    \"\"\"\n",
        "    KL(N(mu_q, var_q) || N(mu_p, var_p)) averaged over batch.\n",
        "    Uses diagonal covariance (logvar = log(sigma^2))\n",
        "\n",
        "    Formula: 0.5 * sum( log(var_p/var_q) + (var_q + (mu_q-mu_p)^2)/var_p - 1 )\n",
        "    \"\"\"\n",
        "    term = logvar_prior - logvar_post + (torch.exp(logvar_post) + (mu_post - mu_prior) ** 2) / torch.exp(logvar_prior) - 1.0\n",
        "    kl = 0.5 * torch.sum(term, dim=1)\n",
        "\n",
        "    return kl.mean()\n",
        "\n",
        "\n",
        "def compute_loss(logits_post, logits_prior, mu_post, logvar_post,\n",
        "                 mu_prior, logvar_prior, hard_target, soft_target,\n",
        "                 epoch, lambda_soft=1.0, lambda_kl=0.1):\n",
        "\n",
        "    # BCE - supervised (posterior only)\n",
        "    if epoch < 2:\n",
        "        loss_bce_post = F.binary_cross_entropy_with_logits(logits_post, hard_target)\n",
        "        loss_bce = loss_bce_post\n",
        "    else:\n",
        "        loss_bce_post = F.binary_cross_entropy_with_logits(logits_post, hard_target)\n",
        "        loss_bce_prior = F.binary_cross_entropy_with_logits(logits_prior, hard_target)\n",
        "        loss_bce = 0.5 * loss_bce_post + 0.5 * loss_bce_prior\n",
        "\n",
        "    #Soft MSE - posterior and prior (sigmoid outputs)\n",
        "    probs_post = torch.sigmoid(logits_post)\n",
        "    loss_soft_post = F.mse_loss(probs_post, soft_target)\n",
        "    if epoch < 1:\n",
        "        loss_soft = loss_soft_post\n",
        "    else:\n",
        "        probs_prior = torch.sigmoid(logits_prior)\n",
        "        loss_soft_prior = F.mse_loss(probs_prior, soft_target)\n",
        "        loss_soft = loss_soft_post + loss_soft_prior\n",
        "\n",
        "    # KL Divergence between posterior and prior\n",
        "    loss_kl = kl_divergence(mu_post, logvar_post, mu_prior, logvar_prior)\n",
        "\n",
        "    # Total loss\n",
        "    total_loss = loss_bce + lambda_soft * loss_soft + lambda_kl * loss_kl\n",
        "\n",
        "    return total_loss, {\"loss_bce\": loss_bce.item(), \"loss_soft\": loss_soft.item(), \"loss_kl\": loss_kl.item()}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76faab78",
      "metadata": {
        "id": "76faab78"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "863520ff",
      "metadata": {
        "id": "863520ff"
      },
      "outputs": [],
      "source": [
        "def freeze_encoder_layers(encoder, freeze_upto: int=0):\n",
        "    roberta_base_model = encoder.encoder\n",
        "\n",
        "    for name, param in roberta_base_model.named_parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    if freeze_upto >= 0:\n",
        "        for layer_idx in range(freeze_upto + 1):\n",
        "            for param in roberta_base_model.encoder.layer[layer_idx].parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    print(f\"\\n\\nFrozen encoder layers - 0 to {freeze_upto}\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "ee546754",
      "metadata": {
        "id": "ee546754"
      },
      "outputs": [],
      "source": [
        "def validate(model, val_dataloader, device, threshold=0.2):\n",
        "    model.eval()\n",
        "    preds_all = []\n",
        "    truths_all = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['atten_masks'].to(device)\n",
        "            hard_target = batch['hard_target'].to(device)\n",
        "\n",
        "            # inference\n",
        "            logits = model.inference(input_ids, attention_mask)\n",
        "\n",
        "            probs = torch.sigmoid(logits).cpu().numpy()\n",
        "            preds = (probs >= threshold).astype(int)\n",
        "            truths = hard_target.cpu().numpy().astype(int)\n",
        "\n",
        "            preds_all.append(preds)\n",
        "            truths_all.append(truths)\n",
        "\n",
        "    preds_all = np.concatenate(preds_all, axis=0)\n",
        "    truths_all = np.concatenate(truths_all, axis=0)\n",
        "\n",
        "    micro_f1 = f1_score(truths_all, preds_all, average='micro', zero_division=0)\n",
        "    macro_f1 = f1_score(truths_all, preds_all, average='macro', zero_division=0)\n",
        "\n",
        "    return {\"micro_f1\": micro_f1, \"macro_f1\": macro_f1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0158faa1",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36de75cf",
      "metadata": {
        "id": "36de75cf"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    model: nn.Module,\n",
        "    train_dataloader: DataLoader,\n",
        "    val_dataloader: DataLoader,\n",
        "    device: torch.device,\n",
        "    epochs: int = 10,\n",
        "    lr_encoder: float = 5e-6,\n",
        "    lr_other: float = 3e-5,\n",
        "    weight_decay: float = 0.01,\n",
        "    warmup_ratio: float = 0.1,\n",
        "    kl_anneal_ratio: float = 0.25,\n",
        "    gradient_accumulation_steps: int = 4,\n",
        "    max_grad_norm: float = 1.0,\n",
        "    use_amp: bool = True,\n",
        "    early_stopping_patience: int = 3,\n",
        "    min_epochs_before_stop: int = 3\n",
        "):\n",
        "    model.to(device)\n",
        "\n",
        "    steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n",
        "    total_steps = steps_per_epoch * epochs\n",
        "\n",
        "    warmup_steps = int(total_steps * warmup_ratio)\n",
        "\n",
        "    global_step = 0\n",
        "    best_val_microF1 = -1.0\n",
        "    epochs_no_improve = 0\n",
        "    current_freeze_config = None\n",
        "\n",
        "    for epoch in range(0, epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        if epoch < 2:\n",
        "            freeze_level = 5\n",
        "            freeze_desc = 'Training last 6 layers'\n",
        "        elif epoch < 4:\n",
        "            freeze_level = 3\n",
        "            freeze_desc = 'Training last 8 layers'\n",
        "        else:\n",
        "            freeze_level = -1\n",
        "            freeze_desc = 'Training all layers'\n",
        "\n",
        "        if freeze_level != current_freeze_config:\n",
        "            freeze_encoder_layers(encoder=model.encoder, freeze_upto=freeze_level)\n",
        "            current_freeze_config = freeze_level\n",
        "\n",
        "            encoder_params, other_params = [], []\n",
        "            for name, p in model.named_parameters():\n",
        "                if not p.requires_grad:\n",
        "                    continue\n",
        "                if 'encoder' in name:\n",
        "                    encoder_params.append(p)\n",
        "                else:\n",
        "                    other_params.append(p)\n",
        "\n",
        "            optimizer = AdamW([\n",
        "                {\"params\": encoder_params, \"lr\": lr_encoder},\n",
        "                {\"params\": other_params, \"lr\": lr_other}\n",
        "            ], weight_decay=weight_decay)\n",
        "\n",
        "            scheduler = get_cosine_schedule_with_warmup(\n",
        "                        optimizer,\n",
        "                        num_warmup_steps=warmup_steps,\n",
        "                        num_training_steps=total_steps\n",
        "                    )\n",
        "\n",
        "        scaler = GradScaler(enabled=(use_amp and device.type =='cuda'))\n",
        "\n",
        "        print(f\"--- Epoch {epoch+1}/{epochs} ({freeze_desc}) ---\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['atten_masks'].to(device)\n",
        "            hard_target = batch['hard_target'].to(device)\n",
        "            soft_target = batch['soft_target'].to(device)\n",
        "\n",
        "            with autocast(device_type='cuda', dtype=torch.float16, enabled=use_amp):\n",
        "                outputs = model(input_ids=input_ids,\n",
        "                                atten_mask=attention_mask,\n",
        "                                emotion_bank=emotion_bank)\n",
        "\n",
        "                logits_post = outputs['logits_post']\n",
        "                logits_prior = outputs['logits_prior']\n",
        "                mu_post, logvar_post = outputs['mu_post'], outputs['logvar_post']\n",
        "                mu_prior, logvar_prior = outputs['mu_prior'], outputs['logvar_prior']\n",
        "\n",
        "                kl_weight = float(torch.sigmoid(torch.tensor((global_step - kl_anneal_steps/2) / (kl_anneal_steps/10))))\n",
        "\n",
        "                total_loss, _ = compute_loss(\n",
        "                    logits_post, logits_prior,\n",
        "                    mu_post, logvar_post, mu_prior, logvar_prior,\n",
        "                    hard_target, soft_target,\n",
        "                    epoch=epoch, lambda_kl=kl_weight\n",
        "                )\n",
        "                total_loss = total_loss / gradient_accumulation_steps\n",
        "\n",
        "            scaler.scale(total_loss).backward()\n",
        "\n",
        "            if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(train_dataloader):\n",
        "                scaler.unscale_(optimizer)\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "            epoch_loss += total_loss.item() * gradient_accumulation_steps\n",
        "        avg_loss = epoch_loss / len(train_dataloader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        metrics = validate(model, val_dataloader, device)\n",
        "        micro_F1 = metrics.get(\"micro_f1\", -1)\n",
        "        macro_F1 = metrics.get(\"macro_f1\", -1)\n",
        "        print(f\"Validation: micro-F1 = {micro_F1:.4f}, macro-F1 = {macro_F1:.4f}\")\n",
        "\n",
        "        if micro_F1 > best_val_microF1:\n",
        "            best_val_microF1 = micro_F1\n",
        "            epochs_no_improve = 0\n",
        "            os.makedirs(\"model_dir\", exist_ok=True)\n",
        "            torch.save(model.state_dict(), os.path.join(\"/content/model_dir\", \"best_model.pt\"))\n",
        "            print(\"Micro-F1 score improved — model saved.\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            print(f\"No improvement for {epochs_no_improve} epoch(s).\")\n",
        "\n",
        "        if epoch + 1 >= min_epochs_before_stop and epochs_no_improve >= early_stopping_patience:\n",
        "            print(\"\\nEarly stopping activated.\")\n",
        "            break\n",
        "\n",
        "    print(f\"\\n\\nTraining completed. Best validation micro-F1 = {best_val_microF1:.4f}\")\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "3Lh9Caf7wguv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Lh9Caf7wguv",
        "outputId": "ea24c0e6-4dc2-4bbb-afce-5a56c92a180d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Oct 29 10:45:53 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   64C    P8             10W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "QAJkae4bxlrT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAJkae4bxlrT",
        "outputId": "a01fe396-bded-41de-d40b-2ddcd69cb6a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())  # Should be True\n",
        "device_name=torch.cuda.get_device_name(0)\n",
        "print(device_name) # Should say 'Tesla T4'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "d7ebd78d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7ebd78d",
        "outputId": "036e3dee-cf80-44be-fcce-aaf2ffc157b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "\n",
            "\n",
            "Frozen encoder layers - 0 to 5\n",
            "\n",
            "\n",
            "--- Epoch 1/10 (Training last 6 layers) ---\n",
            "Epoch 1/10 | Avg Loss: 0.7386\n",
            "Validation: micro-F1 = 0.2257, macro-F1 = 0.0403\n",
            "Micro-F1 score improved — model saved.\n",
            "--- Epoch 2/10 (Training last 6 layers) ---\n",
            "Epoch 2/10 | Avg Loss: 0.4401\n",
            "Validation: micro-F1 = 0.1121, macro-F1 = 0.0135\n",
            "No improvement for 1 epoch(s).\n",
            "\n",
            "\n",
            "Frozen encoder layers - 0 to 3\n",
            "\n",
            "\n",
            "--- Epoch 3/10 (Training last 8 layers) ---\n",
            "Epoch 3/10 | Avg Loss: 0.4251\n",
            "Validation: micro-F1 = 0.2263, macro-F1 = 0.0404\n",
            "Micro-F1 score improved — model saved.\n",
            "--- Epoch 4/10 (Training last 8 layers) ---\n",
            "Epoch 4/10 | Avg Loss: 0.4251\n",
            "Validation: micro-F1 = 0.2263, macro-F1 = 0.0404\n",
            "No improvement for 1 epoch(s).\n",
            "\n",
            "\n",
            "Frozen encoder layers - 0 to -1\n",
            "\n",
            "\n",
            "--- Epoch 5/10 (Training all layers) ---\n",
            "Epoch 5/10 | Avg Loss: 0.4197\n",
            "Validation: micro-F1 = 0.2263, macro-F1 = 0.0404\n",
            "No improvement for 2 epoch(s).\n",
            "--- Epoch 6/10 (Training all layers) ---\n",
            "Epoch 6/10 | Avg Loss: 0.4208\n",
            "Validation: micro-F1 = 0.2263, macro-F1 = 0.0404\n",
            "No improvement for 3 epoch(s).\n",
            "\n",
            "Early stopping activated.\n",
            "\n",
            "\n",
            "Training completed. Best validation micro-F1 = 0.2263\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "EmoAxis(\n",
              "  (encoder): Encoder(\n",
              "    (encoder): RobertaModel(\n",
              "      (embeddings): RobertaEmbeddings(\n",
              "        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "        (token_type_embeddings): Embedding(1, 768)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (encoder): RobertaEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0-11): 12 x RobertaLayer(\n",
              "            (attention): RobertaAttention(\n",
              "              (self): RobertaSdpaSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): RobertaSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): RobertaIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): RobertaOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (pooler): None\n",
              "    )\n",
              "  )\n",
              "  (cross_atten): CrossAttentionModule(\n",
              "    (cross_attn): MultiheadAttention(\n",
              "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "    )\n",
              "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (posterior): PosteriorNetwork(\n",
              "    (mlp): Sequential(\n",
              "      (0): Linear(in_features=768, out_features=512, bias=True)\n",
              "      (1): GELU(approximate='none')\n",
              "      (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (3): Linear(in_features=512, out_features=256, bias=True)\n",
              "      (4): GELU(approximate='none')\n",
              "      (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (mu_posterior): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (logvar_posterior): Linear(in_features=256, out_features=128, bias=True)\n",
              "  )\n",
              "  (prior): PriorNetwork(\n",
              "    (mlp): Sequential(\n",
              "      (0): Linear(in_features=768, out_features=512, bias=True)\n",
              "      (1): GELU(approximate='none')\n",
              "      (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (3): Linear(in_features=512, out_features=256, bias=True)\n",
              "      (4): GELU(approximate='none')\n",
              "      (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (mu_prior): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (logvar_prior): Linear(in_features=256, out_features=128, bias=True)\n",
              "  )\n",
              "  (classifier): Classifier(\n",
              "    (mlp): Sequential(\n",
              "      (0): Linear(in_features=128, out_features=256, bias=True)\n",
              "      (1): GELU(approximate='none')\n",
              "      (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (3): Dropout(p=0.25, inplace=False)\n",
              "      (4): Linear(in_features=256, out_features=128, bias=True)\n",
              "      (5): GELU(approximate='none')\n",
              "      (6): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (7): Dropout(p=0.25, inplace=False)\n",
              "      (8): Linear(in_features=128, out_features=28, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# SET DEVICE\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "train(model=model, train_dataloader=train_dataloader, val_dataloader=val_dataloader, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "2LfGKDpbk5YO",
      "metadata": {
        "id": "2LfGKDpbk5YO"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
