{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25239043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_path = \"C:\\\\Users\\\\subin\\\\.cache\\\\huggingface\\\\hub\\\\models--roberta-base\\\\snapshots\\\\roberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f5dd894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at C:\\Users\\subin\\.cache\\huggingface\\hub\\models--roberta-base\\snapshots\\roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# DOWNLOAD PRETRAINED MODEL AND TOKENIZER\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "roberta_base = AutoModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6abddf4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_base.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "151acca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the pooler layer\n",
    "roberta_base.pooler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3215fcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "df_train = pd.read_csv(r\"D:\\My Projects\\CAPSTONE RESEARCH\\data\\emopillar_train.csv\") \n",
    "df_val = pd.read_csv(r\"D:\\My Projects\\CAPSTONE RESEARCH\\data\\emopillar_validation.csv\") \n",
    "df_test = \"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f39e8822",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmoPillars_Dataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_len = 64\n",
    "        self.target_cols = [str(i) for i in range(28)]\n",
    "        self.soft_target_cols = [str(f\"{i}_exp\") for i in range(28)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return(len(self.data))\n",
    "    \n",
    "    def __get_item__(self, idx):\n",
    "        item = self.data.iloc[idx]\n",
    "        text = str(item.utterance)\n",
    "        encoding = self.tokenizer.encode_plus(text,\n",
    "                                            add_special_tokens=True,\n",
    "                                            truncation=True,\n",
    "                                            return_tensors='pt',\n",
    "                                            max_length=self.max_len,\n",
    "                                            padding='max_length',\n",
    "                                            return_attention_mask=True)\n",
    "        \n",
    "        # convert \"['...']\" → ['...']\n",
    "        labels = ast.literal_eval(item.emotions_used_to_generate_context)\n",
    "        expressiveness = ast.literal_eval(item.expressiveness)\n",
    "        #expressiveness = [float(x) for x in expressiveness]\n",
    "        \n",
    "        target = torch.tensor(item[self.target_cols].values.astype('float32'))\n",
    "        soft_target = torch.tensor(item[self.soft_target_cols].values.astype('float32'))\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"atten_masks\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"label_names\": labels,\n",
    "            \"expressiveness\": expressiveness,\n",
    "            \"hard_target\": target,\n",
    "            \"soft_target\": soft_target\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30f43768",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = EmoPillars_Dataset(df_train, roberta_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e05c804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77477"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a4f9a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([   0,  100,  399,   75, 1057, 7738,    7,   28,   98,  490,    8, 5322,\n",
       "           59,   39, 6453,    4,    2,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1]),\n",
       " 'atten_masks': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'label_names': ['surprise', 'admiration', 'curiosity'],\n",
       " 'expressiveness': [0.8, 0.6, 0.3],\n",
       " 'hard_target': tensor([1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " 'soft_target': tensor([0.6000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8000,\n",
       "         0.0000])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf341534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loaders\n",
    "train_dataloader = DataLoader(EmoPillars_Dataset(df_train, roberta_tokenizer), batch_size=32, num_workers=4)\n",
    "val_dataloader = DataLoader(EmoPillars_Dataset(df_val, roberta_tokenizer), batch_size=32, num_workers=4)\n",
    "#test_dataloader = DataLoader(EmoPillars_Dataset(df_test, roberta_tokenizer), batch_size=32, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da243e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "\n",
    "# Load Precomputed Label Embeddings (JSON → Tensor)\n",
    "with open(\"D:\\My Projects\\CAPSTONE RESEARCH\\label_embeddings.json\", \"r\") as f:\n",
    "    emo_embed_raw = json.load(f)\n",
    "\n",
    "# Convert lists -> torch tensors (float32)\n",
    "emo_embed = {}\n",
    "for k, v in emo_embed_raw.items():\n",
    "    emo_embed[str(k)] = torch.tensor(v, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b87d7b",
   "metadata": {},
   "source": [
    "## Model Architecture Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b7a7075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.amp.autocast_mode import autocast\n",
    "from torch.amp.grad_scaler import GradScaler\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f526781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6bfcadb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "363b53f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Text Encoder:\n",
    "    - Takes in tokenized text (from tokenizer)\n",
    "    - Generates the text embedding vector\n",
    "    \"\"\"\n",
    "    def __init__(self, base_encoder):\n",
    "        super().__init__()\n",
    "        self.encoder = base_encoder\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs: tokenizer output dict (input_ids, attention_mask)\n",
    "        \"\"\"\n",
    "        outputs = self.encoder(**inputs, output_hidden_states=True)\n",
    "        last_hidden_state = outputs.hidden_states[-1]                                                                   # [B, T, H]\n",
    "\n",
    "        atten_mask = inputs['attention_mask']                                                                           # [B, T]\n",
    "        # Mean pooling text_embeddings\n",
    "        atten_mask = atten_mask.unsqueeze(-1).float()\n",
    "        pooled_text_emb = (last_hidden_state * atten_mask).sum(dim=1) / atten_mask.sum(dim=1).clamp(min=1e-9)           # [B, H]\n",
    "\n",
    "        return {\n",
    "            \"text_embed\": pooled_text_emb,\n",
    "            \"last_hidden_state\": last_hidden_state,\n",
    "            \"atten_mask\": atten_mask\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3171d01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionModule(nn.Module):\n",
    "    def __init__(self, label_embedding_dict):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.label_embedding_dict = {\n",
    "            k: v.clone().detach() for k, v in label_embedding_dict.items()\n",
    "        }\n",
    "        # Multi-head cross attention \n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "            embed_dim=768,\n",
    "            num_heads=6,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(768)\n",
    "\n",
    "    def forward(self, encoder_out, emotion_labels, expressiveness):\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        emotion_emb = []\n",
    "        for labels, weights in zip(emotion_labels, expressiveness):\n",
    "            # stack embeddings of all emotion descriptions\n",
    "            emb_list = [self.label_embedding_dict[lbl].to(device) for lbl in labels]\n",
    "            emb_stack = torch.stack(emb_list, dim=0)                                                       # [num_labels, H]\n",
    "\n",
    "            # normalize weights → weighted mean\n",
    "            w = torch.tensor(weights, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "            w = w / w.sum()\n",
    "            weighted_emb = (emb_stack * w).sum(dim=0)                                                      # [H]\n",
    "            emotion_emb.append(weighted_emb)\n",
    "\n",
    "        emotion_emb = torch.stack(emotion_emb, dim=0).unsqueeze(1)                                         # [B, 1, H]\n",
    "\n",
    "        # cross-attention (query=text, key/value=emotion)\n",
    "        attn_out, _ = self.cross_attn(\n",
    "            query = encoder_out[\"last_hidden_state\"],                     # [B, T, H]   \n",
    "            key = emotion_emb,                                            # [B, 1, H]\n",
    "            value = emotion_emb                                           # [B, 1, H]\n",
    "        )\n",
    "\n",
    "        # Fuse and pool\n",
    "        fused_hidden_state = encoder_out[\"last_hidden_state\"] + attn_out\n",
    "        atten_mask = encoder_out[\"atten_mask\"]\n",
    "        fused_emo_text_emb = (fused_hidden_state * atten_mask).sum(dim=1) / atten_mask.sum(dim=1).clamp(min=1e-9)\n",
    "        fused_emo_text_emb = self.layer_norm(fused_emo_text_emb)\n",
    "\n",
    "        return fused_emo_text_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ec475236",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosteriorNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Learns an emotion-aware posterior distribution q(z | x, e)\n",
    "    over latent space using fused encoder output.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=768, latent_dim=128):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(256)\n",
    "        )\n",
    "        self.mu_posterior = nn.Linear(256, latent_dim)\n",
    "        self.logvar_posterior = nn.Linear(256, latent_dim)\n",
    "\n",
    "    def forward(self, fused_emo_text_emb):\n",
    "        h = self.mlp(fused_emo_text_emb)\n",
    "        mu_post = self.mu_posterior(h)\n",
    "        logvar_post = torch.clamp(self.logvar_posterior(h), min=-10, max=10)\n",
    "\n",
    "        # Reparameterization trick: sample z ~ N(mu, sigma^2)\n",
    "        std = torch.exp(0.5 * logvar_post)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu_post + eps * std\n",
    "        \n",
    "        return z, mu_post, logvar_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4efaefc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriorNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Learns a prior distribution p(z | x)\n",
    "    based only on text (without emotion labels).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=768, latent_dim=128):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(256)\n",
    "        )\n",
    "        self.mu_prior = nn.Linear(256, latent_dim)\n",
    "        self.logvar_prior = nn.Linear(256, latent_dim)\n",
    "\n",
    "    def forward(self, text_emb):\n",
    "        h = self.mlp(text_emb)\n",
    "        mu_prior = self.mu_prior(h)\n",
    "        logvar_prior = torch.clamp(self.logvar_prior(h), min=-10, max=10)\n",
    "        \n",
    "        return mu_prior, logvar_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8402d8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Final shared emotion classifier layer\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=128, num_classes=28):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.mlp(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7d61c90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Model class\n",
    "class EmoAxis(nn.Module):\n",
    "    def __init__(self, encoder, cross_atten_module, posterior_net, prior_net, classifier):\n",
    "        \"\"\"\n",
    "        Architecture:\n",
    "        - EncoderBlock   → produces text and fused(text+emotion) embeddings\n",
    "        - PosteriorNet   → q(z|x,e)\n",
    "        - PriorNet       → p(z|x)\n",
    "        - EmotionClassifier → predicts emotions from latent z\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.cross_atten = cross_atten_module\n",
    "        self.posterior = posterior_net\n",
    "        self.prior = prior_net\n",
    "        self.classifier = classifier\n",
    "\n",
    "\n",
    "    def total_params(self):\n",
    "        \"\"\"Utility function to check trainable vs total params.\"\"\"\n",
    "        total = sum(p.numel() for p in self.parameters())\n",
    "        trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print(f\"Total parameters: {total:,}\")\n",
    "        print(f\"Trainable parameters: {trainable:,}\")\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, atten_mask, emotion_labels, expressiveness):\n",
    "        # Encoder\n",
    "        encoder_outputs = self.encoder(\n",
    "            inputs = {\"input_ids\": input_ids, \"attention_mask\": atten_mask},\n",
    "        )\n",
    "        text_emb = encoder_outputs[\"text_embed\"]                  # [B, H]\n",
    "\n",
    "        # Cross Attention\n",
    "        fused_emb = self.cross_atten(encoder_outputs, emotion_labels, expressiveness)    # [B, H]\n",
    "\n",
    "        # Posterior Net\n",
    "        z_post, mu_post, logvar_post = self.posterior(fused_emb)\n",
    "        \n",
    "        # Prior Net\n",
    "        mu_prior, logvar_prior = self.prior(text_emb)\n",
    "        z_prior = mu_prior + torch.exp(0.5 * logvar_prior) * torch.randn_like(mu_prior)\n",
    "\n",
    "        # 4. Classifier\n",
    "        logits_post = self.classifier(z_post)           # from sampled posterior\n",
    "        logits_prior = self.classifier(z_prior)         # from prior mean\n",
    "\n",
    "        return {\n",
    "            \"mu_post\": mu_post,\n",
    "            \"logvar_post\": logvar_post,\n",
    "            \"mu_prior\": mu_prior,\n",
    "            \"logvar_prior\": logvar_prior,\n",
    "            \"logits_post\": logits_post,\n",
    "            \"logits_prior\": logits_prior\n",
    "        }\n",
    "    \n",
    "\n",
    "    def inference(self, input_ids, atten_mask):\n",
    "        \"\"\"\n",
    "        Predict emotions from raw text (inference using prior p(z|x)).\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            encoder_outputs = self.encoder(\n",
    "                inputs = {\"input_ids\": input_ids, \"attention_mask\": atten_mask},\n",
    "            )\n",
    "            # get only text embeddings \n",
    "            text_emb = encoder_outputs[\"text_embed\"]\n",
    "\n",
    "            # latent mean from prior\n",
    "            mu_prior, logvar_prior = self.prior(text_emb)\n",
    "\n",
    "            # classification\n",
    "            logits = self.classifier(mu_prior)\n",
    "\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c9f26c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 127,674,012\n",
      "Trainable parameters: 127,674,012\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(base_encoder=roberta_base)\n",
    "\n",
    "cross_atten_module = CrossAttentionModule(label_embedding_dict=emo_embed)\n",
    "\n",
    "posterior_net = PosteriorNetwork()\n",
    "prior_net = PriorNetwork()\n",
    "classifier = Classifier()\n",
    "\n",
    "# Initialize model\n",
    "model = EmoAxis(\n",
    "    encoder=encoder,\n",
    "    cross_atten_module=cross_atten_module,\n",
    "    posterior_net=posterior_net,\n",
    "    prior_net=prior_net,\n",
    "    classifier=classifier\n",
    ")\n",
    "\n",
    "model.total_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2db152c",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "69e3c2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(mu_post, logvar_post, mu_prior, logvar_prior):\n",
    "    \"\"\"\n",
    "    KL(N(mu_q, var_q) || N(mu_p, var_p)) averaged over batch.\n",
    "    Uses diagonal covariance (logvar = log(sigma^2))\n",
    "\n",
    "    Formula: 0.5 * sum( log(var_p/var_q) + (var_q + (mu_q-mu_p)^2)/var_p - 1 )\n",
    "    \"\"\"\n",
    "    term = logvar_prior - logvar_post + (torch.exp(logvar_post) + (mu_post - mu_prior) ** 2) / torch.exp(logvar_prior) - 1.0\n",
    "    kl = 0.5 * torch.sum(term, dim=1)\n",
    "    \n",
    "    return kl.mean()\n",
    "    \n",
    "\n",
    "def compute_loss(logits_post, logits_prior, mu_post, logvar_post,\n",
    "                 mu_prior, logvar_prior, hard_target, soft_target,\n",
    "                 epoch, lambda_soft=1.0, lambda_kl=0.1):\n",
    "\n",
    "    # BCE - supervised (posterior only)\n",
    "    loss_bce = F.binary_cross_entropy_with_logits(logits_post, hard_target)\n",
    "\n",
    "    #Soft MSE - posterior and prior (sigmoid outputs)\n",
    "    probs_post = torch.sigmoid(logits_post)\n",
    "    loss_soft_post = F.mse_loss(probs_post, soft_target) \n",
    "    if epoch < 1:\n",
    "        loss_soft = loss_soft_post\n",
    "    else:\n",
    "        probs_prior = torch.sigmoid(logits_prior)\n",
    "        loss_soft_prior = F.mse_loss(probs_prior, soft_target)\n",
    "        loss_soft = loss_soft_post + loss_soft_prior\n",
    "\n",
    "    # KL Divergence between posterior and prior\n",
    "    loss_kl = kl_divergence(mu_post, logvar_post, mu_prior, logvar_prior)\n",
    "\n",
    "    # Total loss  \n",
    "    total_loss = loss_bce + lambda_soft * loss_soft + lambda_kl * loss_kl\n",
    "    \n",
    "    return total_loss, {\"loss_bce\": loss_bce.item(), \"loss_soft\": loss_soft.item(), \"loss_kl\": loss_kl.item()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76faab78",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "863520ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_encoder_layers(encoder=roberta_base, freeze_upto: int=0):\n",
    "    \"\"\"\n",
    "    Freezes encoder layers from 0 → freeze_upto (inclusive).\n",
    "    Example: freeze_upto = 5 → freeze encoder.layer[0] ... encoder.layer[5].\n",
    "    \"\"\"\n",
    "    for name, param in encoder.encoder.named_parameters():\n",
    "        param.requires_grad = True                      # unfreeze all layers\n",
    "\n",
    "    # freeze\n",
    "    for layer_idx in range(freeze_upto + 1):\n",
    "        for param in encoder.encoder.layer[layer_idx].parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    print(f\"Frozen encoder layers - 0 to {freeze_upto}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ee546754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_dataloader, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    preds_all = []\n",
    "    truths_all = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['atten_masks'].to(device)\n",
    "            hard_target = batch['hard_target'].to(device)\n",
    "\n",
    "            # inference\n",
    "            logits = model.inference(input_ids, attention_mask)\n",
    "\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            preds = (probs >= threshold).astype(int)\n",
    "            truths = hard_target.cpu().numpy().astype(int)\n",
    "\n",
    "            preds_all.append(preds)\n",
    "            truths_all.append(truths)\n",
    "\n",
    "    preds_all = np.concatenate(preds_all, axis=0)\n",
    "    truths_all = np.concatenate(truths_all, axis=0)\n",
    "\n",
    "    micro_f1 = f1_score(truths_all, preds_all, average='micro', zero_division=0)\n",
    "    macro_f1 = f1_score(truths_all, preds_all, average='macro', zero_division=0)\n",
    "    \n",
    "    return {\"micro_f1\": micro_f1, \"macro_f1\": macro_f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "36de75cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main model training function\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    train_dataloader: DataLoader,\n",
    "    val_dataloader: DataLoader,\n",
    "    device: torch.device,\n",
    "    epochs: int = 10,\n",
    "    lr_encoder: float = 2e-5,\n",
    "    lr_other: float = 1e-4,\n",
    "    weight_decay: float = 0.01,\n",
    "    warmup_ratio: float = 0.1,\n",
    "    kl_anneal_ratio: float = 0.25,\n",
    "    gradient_accumulation_steps: int = 4,\n",
    "    max_grad_norm: float = 1.0,\n",
    "    use_amp: bool = True,\n",
    "    early_stopping_patience: int = 3,\n",
    "    min_epochs_before_stop: int = 3\n",
    "):\n",
    "    model.to(device)\n",
    "\n",
    "    # Separate RoBERTa-encoder params &  other_params for different learning rate\n",
    "    encoder_params, other_params = [], []\n",
    "    for name, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        if 'encoder' in name:\n",
    "            encoder_params.append(p)\n",
    "        else:\n",
    "            other_params.append(p)\n",
    "\n",
    "    optimizer = AdamW([\n",
    "        {\"params\": encoder_params, \"lr\": lr_encoder},\n",
    "        {\"params\": other_params, \"lr\": lr_other}\n",
    "    ], weight_decay=weight_decay)\n",
    "\n",
    "    steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n",
    "    total_steps = steps_per_epoch * epochs\n",
    "\n",
    "    warmup_steps = int(total_steps * warmup_ratio)\n",
    "    kl_anneal_steps = int(total_steps * kl_anneal_ratio)\n",
    "\n",
    "    # Build scheduler based on step count\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer,\n",
    "                num_warmup_steps=warmup_steps,\n",
    "                num_training_steps=total_steps\n",
    "            )\n",
    "\n",
    "    scaler = GradScaler(enabled=(use_amp and device.type =='cuda'))\n",
    "\n",
    "    global_step = 0\n",
    "    best_val_microF1 = -1.0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(0, epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # gradual encoder layers freezing\n",
    "        if epoch < 2:\n",
    "            freeze_encoder_layers(freeze_upto=5)         # Train last 6 layers only\n",
    "        elif epoch < 4:\n",
    "            freeze_encoder_layers(freeze_upto=3)         # Train last 8 layers\n",
    "        else:\n",
    "            freeze_encoder_layers(freeze_upto=-1)        # Train all layers\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            atten_mask = batch['atten_masks'].to(device)\n",
    "            labels = batch['label_names']\n",
    "            expressiveness = batch['expressiveness']\n",
    "            hard_target = batch['hard_target'].to(device)\n",
    "            soft_target = batch['soft_target'].to(device)\n",
    "\n",
    "            with autocast(device_type='cuda', dtype=torch.bfloat16, enabled=use_amp):\n",
    "                outputs = model(input_ids=input_ids,\n",
    "                                atten_mask=atten_mask,\n",
    "                                emotion_labels=labels,\n",
    "                                expressiveness=expressiveness)\n",
    "\n",
    "                logits_post = outputs['logits_post']\n",
    "                logits_prior = outputs['logits_prior']\n",
    "                mu_post, logvar_post = outputs['mu_post'], outputs['logvar_post']\n",
    "                mu_prior, logvar_prior = outputs['mu_prior'], outputs['logvar_prior']\n",
    "\n",
    "                # KL weight annealing schedule\n",
    "                kl_weight = min(1.0, global_step / max(1, kl_anneal_steps))\n",
    "\n",
    "                total_loss, _ = compute_loss(\n",
    "                    logits_post, logits_prior,\n",
    "                    mu_post, logvar_post, mu_prior, logvar_prior,\n",
    "                    hard_target, soft_target,\n",
    "                    epoch=epoch, lambda_kl=kl_weight\n",
    "                )\n",
    "                # Normalize loss for gradient accumulation\n",
    "                total_loss = total_loss / gradient_accumulation_steps\n",
    "\n",
    "            # Backward\n",
    "            scaler.scale(total_loss).backward()\n",
    "\n",
    "            if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(train_dataloader):\n",
    "                # gradient clipping\n",
    "                scaler.unscale_(optimizer)\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "            epoch_loss += total_loss.item() * gradient_accumulation_steps \n",
    "            avg_loss = epoch_loss / len(train_dataloader)\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Avg Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        metrics = validate(model, val_dataloader, device)\n",
    "        micro_F1 = metrics.get(\"micro_f1\", -1)\n",
    "        macro_F1 = metrics.get(\"macro_f1\", -1)\n",
    "        print(f\"Validation: micro-F1 = {micro_F1:.4f}, macro-F1 = {macro_F1:.4f}\")\n",
    "\n",
    "        if micro_F1 > best_val_microF1:\n",
    "            best_val_microF1 = micro_F1\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), os.path.join(\"model_dir\", \"best_model.pt\"))                # ENTER MODEL DIR PATH FOR SAVING MODEL\n",
    "            print(\"Micro-F1 score improved — model saved.\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"No improvement for {epochs_no_improve} epoch(s).\")\n",
    "\n",
    "        # Early Stopping\n",
    "        if epoch + 1 >= min_epochs_before_stop and epochs_no_improve >= early_stopping_patience:\n",
    "            print(\"\\nEarly stopping activated.\")\n",
    "            break\n",
    "\n",
    "    print(f\"\\n\\nTraining completed. Best validation micro-F1 = {best_val_microF1:.4f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ebd78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen encoder layers - 0 to 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SET DEVICE \n",
    "train(model=model, train_dataloader=train_dataloader, val_dataloader=val_dataloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e219011",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
