{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ju_TEBblVYHL",
      "metadata": {
        "id": "ju_TEBblVYHL"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import os\n",
        "import math\n",
        "import json\n",
        "import gc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.amp.autocast_mode import autocast\n",
        "from torch.amp.grad_scaler import GradScaler\n",
        "from transformers import AutoTokenizer, AutoModel, get_cosine_schedule_with_warmup\n",
        "from sklearn.metrics import f1_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5be8e02",
      "metadata": {
        "id": "a5be8e02"
      },
      "outputs": [],
      "source": [
        "# DOWNLOAD PRETRAINED MODEL AND TOKENIZER\n",
        "roberta_tokenizer = AutoTokenizer.from_pretrained(\"alex-shvets/roberta-large-emopillars-contextless\")\n",
        "roberta_base = AutoModel.from_pretrained(\"alex-shvets/roberta-large-emopillars-contextless\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZoN68wOcVkrG",
      "metadata": {
        "id": "ZoN68wOcVkrG"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"using device: \", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0c37050",
      "metadata": {
        "id": "d0c37050"
      },
      "outputs": [],
      "source": [
        "# Remove the pooler layer\n",
        "roberta_base.pooler = None\n",
        "roberta_base.gradient_checkpointing_enable()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vGL3WLsNJdre",
      "metadata": {
        "id": "vGL3WLsNJdre"
      },
      "outputs": [],
      "source": [
        "HIDDEN_SIZE = roberta_base.config.hidden_size\n",
        "print(\"Hidden size: \", HIDDEN_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb384cbb",
      "metadata": {
        "id": "bb384cbb"
      },
      "outputs": [],
      "source": [
        "train_path = r\"C:\\Users\\arnab\\Desktop\\emopillar_train.csv\"\n",
        "val_path = r\"C:\\Users\\arnab\\Desktop\\emopillar_validation.csv\"\n",
        "test_path = r\"C:\\Users\\arnab\\Desktop\\emopillar_test.csv\"\n",
        "\n",
        "df_train = pd.read_csv(train_path)\n",
        "df_val = pd.read_csv(val_path)\n",
        "df_test = pd.read_csv(test_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2637acde",
      "metadata": {
        "id": "2637acde"
      },
      "outputs": [],
      "source": [
        "class EmoPillars_Dataset(Dataset):\n",
        "    def __init__(self, data: pd.DataFrame, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = data\n",
        "        self.max_len = 128\n",
        "        self.target_cols = [str(i) for i in range(28)]\n",
        "        self.soft_target_cols = [str(f\"{i}_exp\") for i in range(28)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return(len(self.data))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data.iloc[idx]\n",
        "        text = str(item.utterance)\n",
        "        encoding = self.tokenizer.encode_plus(text,\n",
        "                                            add_special_tokens=True,\n",
        "                                            truncation=True,\n",
        "                                            return_tensors='pt',\n",
        "                                            max_length=self.max_len,\n",
        "                                            padding='max_length',\n",
        "                                            return_attention_mask=True)\n",
        "\n",
        "        target = torch.tensor(item[self.target_cols].values.astype('float32'))\n",
        "        soft_target = torch.tensor(item[self.soft_target_cols].values.astype('float32'))\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
        "            \"atten_masks\": encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"hard_target\": target,\n",
        "            \"soft_target\": soft_target\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "823ababa",
      "metadata": {
        "id": "823ababa"
      },
      "outputs": [],
      "source": [
        "data = EmoPillars_Dataset(df_train, roberta_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f2fd900",
      "metadata": {
        "id": "9f2fd900"
      },
      "outputs": [],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb0f44ab",
      "metadata": {
        "id": "cb0f44ab"
      },
      "outputs": [],
      "source": [
        "data.__getitem__(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbdcaa4c",
      "metadata": {
        "id": "bbdcaa4c"
      },
      "outputs": [],
      "source": [
        "# Data Loaders\n",
        "train_dataloader = DataLoader(EmoPillars_Dataset(df_train, roberta_tokenizer), batch_size=64, num_workers=4, shuffle=True)\n",
        "val_dataloader = DataLoader(EmoPillars_Dataset(df_val, roberta_tokenizer), batch_size=64, num_workers=4)\n",
        "test_dataloader = DataLoader(EmoPillars_Dataset(df_test, roberta_tokenizer), batch_size=64, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4977a745",
      "metadata": {
        "id": "4977a745"
      },
      "outputs": [],
      "source": [
        "with open(r\"C:\\Users\\arnab\\Desktop\\label_embeddings.json\", \"r\") as f:\n",
        "    label_embeddings = json.load(f)\n",
        "\n",
        "emo_emb=[]\n",
        "for k, v in label_embeddings.items():\n",
        "    emb = torch.tensor(v, dtype=torch.float32)\n",
        "    emo_emb.append(emb)\n",
        "\n",
        "emotion_bank = torch.cat(emo_emb, dim=0).to(device)\n",
        "print(emotion_bank.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f6fd0d5",
      "metadata": {
        "id": "0f6fd0d5"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Text Encoder:\n",
        "    - Takes in tokenized text (from tokenizer)\n",
        "    - Generates the text embedding vector\n",
        "    \"\"\"\n",
        "    def __init__(self, base_encoder):\n",
        "        super().__init__()\n",
        "        self.encoder = base_encoder\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        inputs: tokenizer output dict (input_ids, attention_mask)\n",
        "        \"\"\"\n",
        "        outputs = self.encoder(**inputs, output_hidden_states=True)\n",
        "        last_hidden_state = outputs.hidden_states[-1]                                                                   # [B, T, H]\n",
        "\n",
        "        atten_mask = inputs['attention_mask']                                                                           # [B, T]\n",
        "        # Mean pooling text_embeddings\n",
        "        atten_mask = atten_mask.unsqueeze(-1).float()\n",
        "        pooled_text_emb = (last_hidden_state * atten_mask).sum(dim=1) / atten_mask.sum(dim=1).clamp(min=1e-9)           # [B, H]\n",
        "\n",
        "        return {\n",
        "            \"text_embed\": pooled_text_emb,\n",
        "            \"last_hidden_state\": last_hidden_state,\n",
        "            \"atten_mask\": atten_mask\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "O0wto7oBGYzK",
      "metadata": {
        "id": "O0wto7oBGYzK"
      },
      "outputs": [],
      "source": [
        "class CrossAttentionModule(nn.Module):\n",
        "    def __init__(self, emotion_bank, hidden_size=HIDDEN_SIZE):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.emo_bank = emotion_bank\n",
        "        self.register_buffer(\"emotion_bank\", emotion_bank)\n",
        "\n",
        "        # Multi-head cross attention\n",
        "        self.cross_attn = nn.MultiheadAttention(\n",
        "            embed_dim=self.hidden_size,\n",
        "            num_heads=16,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.rms_norm = nn.RMSNorm(hidden_size)\n",
        "\n",
        "    def forward(self, encoder_out):\n",
        "        B = encoder_out[\"last_hidden_state\"].size(0)\n",
        "        emo_bank = self.emo_bank.unsqueeze(0).expand(B, -1, -1)\n",
        "\n",
        "        # cross-attention (query=text, key/value=emotion)\n",
        "        attn_out, _ = self.cross_attn(\n",
        "            query = encoder_out[\"last_hidden_state\"],                        # [B, T, HIDDEN_SIZE]\n",
        "            key = emo_bank,                                                  # [B, 28, HIDDEN_SIZE] (Key/Value)\n",
        "            value = emo_bank,                                                # [B, 28, HIDDEN_SIZE]\n",
        "        )\n",
        "\n",
        "        # Fuse and pool\n",
        "        fused_hidden_state = encoder_out[\"last_hidden_state\"] + attn_out\n",
        "        atten_mask = encoder_out[\"atten_mask\"]\n",
        "        fused_emo_text_emb = (fused_hidden_state * atten_mask).sum(dim=1) / atten_mask.sum(dim=1).clamp(min=1e-9)\n",
        "        fused_emo_text_emb = self.rms_norm(fused_emo_text_emb)\n",
        "\n",
        "        return fused_emo_text_emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d78666a",
      "metadata": {
        "id": "5d78666a"
      },
      "outputs": [],
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_dim=HIDDEN_SIZE, num_classes=28):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.RMSNorm(512),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, h):\n",
        "        return self.mlp(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0cedef9",
      "metadata": {
        "id": "b0cedef9"
      },
      "outputs": [],
      "source": [
        "# Main Model class\n",
        "class EmoAxis(nn.Module):\n",
        "    def __init__(self, encoder, cross_atten_module, classifier):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.cross_atten = cross_atten_module\n",
        "        self.classifier = classifier\n",
        "\n",
        "\n",
        "    def total_params(self):\n",
        "        \"\"\"Utility function to check trainable vs total params.\"\"\"\n",
        "        total = sum(p.numel() for p in self.parameters())\n",
        "        trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "        print(f\"Total parameters: {total:,}\")\n",
        "        print(f\"Trainable parameters: {trainable:,}\")\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, atten_mask):\n",
        "        # Encoder\n",
        "        outputs = self.encoder(\n",
        "            inputs = {\"input_ids\": input_ids, \"attention_mask\": atten_mask}\n",
        "            )\n",
        "        # Cross Attention\n",
        "        fused_emb = self.cross_atten(outputs)\n",
        "        # Classifier\n",
        "        logits = self.classifier(fused_emb)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "    def infer(self, input_ids, atten_mask):\n",
        "        \"\"\"\n",
        "        Predict emotions from raw text (inference using prior p(z|x)).\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            logits = self.forward(input_ids, atten_mask)\n",
        "            return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e29fcf6",
      "metadata": {
        "id": "4e29fcf6"
      },
      "outputs": [],
      "source": [
        "encoder = Encoder(base_encoder=roberta_base)\n",
        "\n",
        "cross_atten_module = CrossAttentionModule(emotion_bank=emotion_bank, hidden_size=HIDDEN_SIZE)\n",
        "classifier = Classifier(input_dim=HIDDEN_SIZE)\n",
        "\n",
        "# Initialize model\n",
        "model = EmoAxis(\n",
        "    encoder=encoder,\n",
        "    cross_atten_module=cross_atten_module,\n",
        "    classifier=classifier\n",
        ")\n",
        "\n",
        "model.total_params()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cee0c585",
      "metadata": {
        "id": "cee0c585"
      },
      "outputs": [],
      "source": [
        "def freeze_encoder_layers(encoder, freeze_upto: int=0):\n",
        "    roberta_base_model = encoder.encoder\n",
        "\n",
        "    for name, param in roberta_base_model.named_parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    if freeze_upto >= 0:\n",
        "        for layer_idx in range(freeze_upto + 1):\n",
        "            for param in roberta_base_model.encoder.layer[layer_idx].parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    print(f\"\\n\\nFrozen encoder layers - 0 to {freeze_upto}\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7c191d9",
      "metadata": {
        "id": "c7c191d9"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, val_dataloader, device, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Evaluate the model on validation set.\n",
        "    Returns:\n",
        "        - avg_val_loss: per-sample BCE loss\n",
        "        - micro_f1, macro_f1: multi-label F1 scores\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    loss_func = nn.BCEWithLogitsLoss()\n",
        "    val_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    preds_all = []\n",
        "    truths_all = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['atten_masks'].to(device)\n",
        "            hard_target = batch['hard_target'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model.infer(input_ids, attention_mask)\n",
        "            loss = loss_func(logits, hard_target)\n",
        "\n",
        "            batch_size = input_ids.size(0)\n",
        "            val_loss += loss.item() * batch_size\n",
        "            total_samples += batch_size\n",
        "\n",
        "            # Predictions\n",
        "            probs = torch.sigmoid(logits)\n",
        "            preds = (probs >= threshold).int()\n",
        "\n",
        "            preds_all.append(preds.cpu())\n",
        "            truths_all.append(hard_target.cpu().int())\n",
        "\n",
        "    # Concatenate all batches\n",
        "    preds_all = torch.cat(preds_all, dim=0).numpy()\n",
        "    truths_all = torch.cat(truths_all, dim=0).numpy()\n",
        "\n",
        "    # Compute metrics\n",
        "    avg_val_loss = val_loss / total_samples\n",
        "    micro_f1 = f1_score(truths_all, preds_all, average='micro', zero_division=0)\n",
        "    macro_f1 = f1_score(truths_all, preds_all, average='macro', zero_division=0)\n",
        "\n",
        "    return {\n",
        "        \"avg_val_loss\": avg_val_loss,\n",
        "        \"micro_f1\": micro_f1,\n",
        "        \"macro_f1\": macro_f1\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41e9537a",
      "metadata": {
        "id": "41e9537a"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    model: torch.nn.Module,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    device: torch.device,\n",
        "    epochs: int = 12,\n",
        "    lr_encoder: float = 2.5e-6,\n",
        "    lr_other: float = 2e-5,\n",
        "    weight_decay: float = 0.001,\n",
        "    warmup_ratio: float = 0.07,\n",
        "    gradient_accumulation_steps: int = 4,\n",
        "    max_grad_norm: float = 1.0,\n",
        "    use_amp: bool = True,\n",
        "    early_stopping_patience: int = 3,\n",
        "    min_epochs_before_stop: int = 3,\n",
        "    use_grad_checkpointing: bool = True   # Optional memory optimization\n",
        "):\n",
        "    model.to(device)\n",
        "\n",
        "    # --- Optional: enable gradient checkpointing ---\n",
        "    if use_grad_checkpointing and hasattr(model.encoder.encoder, \"gradient_checkpointing_enable\"):\n",
        "        print(\"âœ… Gradient checkpointing enabled for encoder.\")\n",
        "        model.encoder.encoder.gradient_checkpointing_enable()\n",
        "\n",
        "    # --- Scheduler setup ---\n",
        "    steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n",
        "    total_steps = steps_per_epoch * epochs\n",
        "    warmup_steps = int(total_steps * warmup_ratio)\n",
        "\n",
        "    loss_func = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    global_step = 0\n",
        "    best_val_microF1 = -1.0\n",
        "    epochs_no_improve = 0\n",
        "    current_freeze_config = None\n",
        "\n",
        "    history = {\"train_loss\": [], \"val_loss\": [], \"micro_f1\": [], \"macro_f1\": []}\n",
        "\n",
        "    optimizer = None\n",
        "    scheduler = None\n",
        "    scaler = GradScaler(enabled=(use_amp and device.type == 'cuda'))   # moved scaler outside loop\n",
        "    # --- Training Loop ---\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        total_samples = 0\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        # Progressive unfreezing for 24-layer RoBERTa\n",
        "        # Stage 1: train only classifier (0-23 frozen)\n",
        "        # Stage 2: unfreeze top 6 layers\n",
        "        # Stage 3: unfreeze 12 layers\n",
        "        # Stage 4+: fully unfreeze\n",
        "        if epoch < 3:\n",
        "            freeze_level = 23\n",
        "        elif epoch < 6:\n",
        "            freeze_level = 17\n",
        "        elif epoch < 9:\n",
        "            freeze_level = 11\n",
        "        else:\n",
        "            freeze_level = -1\n",
        "\n",
        "        # Reset optimizer/scheduler only if freeze config changed\n",
        "        if freeze_level != current_freeze_config:\n",
        "            freeze_encoder_layers(model.encoder, freeze_upto=freeze_level)\n",
        "            current_freeze_config = freeze_level\n",
        "\n",
        "            encoder_params, other_params = [], []\n",
        "            for name, p in model.named_parameters():\n",
        "                if not p.requires_grad:\n",
        "                    continue\n",
        "                if 'encoder' in name:\n",
        "                    encoder_params.append(p)\n",
        "                else:\n",
        "                    other_params.append(p)\n",
        "\n",
        "            optimizer = AdamW([\n",
        "                {\"params\": encoder_params, \"lr\": lr_encoder},\n",
        "                {\"params\": other_params, \"lr\": lr_other}\n",
        "            ], weight_decay=weight_decay)\n",
        "\n",
        "            scheduler = get_cosine_schedule_with_warmup(\n",
        "                optimizer,\n",
        "                num_warmup_steps=warmup_steps,\n",
        "                num_training_steps=total_steps\n",
        "            )\n",
        "\n",
        "        # scaler = GradScaler(enabled=(use_amp and device.type == 'cuda'))\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # --- Batch Loop ---\n",
        "        progress_bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "        for step, batch in progress_bar:\n",
        "            input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
        "            attention_mask = batch['atten_masks'].to(device, non_blocking=True)\n",
        "            hard_target = batch['hard_target'].to(device, non_blocking=True)\n",
        "\n",
        "            batch_size = input_ids.size(0)\n",
        "\n",
        "            with autocast(device_type='cuda', dtype=torch.float16, enabled=use_amp):\n",
        "                logits = model(input_ids=input_ids, atten_mask=attention_mask)\n",
        "                loss = loss_func(logits, hard_target)\n",
        "                total_loss = loss / gradient_accumulation_steps\n",
        "\n",
        "            scaler.scale(total_loss).backward()\n",
        "\n",
        "            if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(train_dataloader):\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "                global_step += 1\n",
        "\n",
        "            epoch_loss += loss.item() * batch_size\n",
        "            total_samples += batch_size\n",
        "\n",
        "            # ðŸ§¹ Periodic GPU cleanup\n",
        "            if step % 100 == 0 and step != 0:\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "        avg_train_loss = epoch_loss / total_samples\n",
        "        history[\"train_loss\"].append(avg_train_loss)\n",
        "        print(f\"\\n| Epoch {epoch+1}/{epochs} | Avg Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # --- Validation ---\n",
        "        val_metrics = evaluate(model, val_dataloader, device)\n",
        "        avg_val_loss = val_metrics.get(\"avg_val_loss\", None)\n",
        "        micro_F1 = val_metrics.get(\"micro_f1\", -1)\n",
        "        macro_F1 = val_metrics.get(\"macro_f1\", -1)\n",
        "\n",
        "        history[\"val_loss\"].append(avg_val_loss)\n",
        "        history[\"micro_f1\"].append(micro_F1)\n",
        "        history[\"macro_f1\"].append(macro_F1)\n",
        "\n",
        "        print(f\"Validation | Avg Loss: {avg_val_loss:.4f}; Micro-F1: {micro_F1:.4f}; Macro-F1: {macro_F1:.4f}\")\n",
        "\n",
        "        # --- Early Stopping ---\n",
        "        if micro_F1 > best_val_microF1:\n",
        "            best_val_microF1 = micro_F1\n",
        "            epochs_no_improve = 0\n",
        "            os.makedirs(\"model\", exist_ok=True)\n",
        "            torch.save(model.state_dict(), os.path.join(\"model\", \"best_model.pt\"))\n",
        "            print(\"âœ… Micro-F1 improved â€” model saved.\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            print(f\"âš ï¸ No improvement for {epochs_no_improve} epoch(s).\")\n",
        "\n",
        "        if epoch + 1 >= min_epochs_before_stop and epochs_no_improve >= early_stopping_patience:\n",
        "            print(\"\\nâ›” Early stopping activated.\")\n",
        "            break\n",
        "\n",
        "        # Cleanup after each epoch\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    print(f\"\\n\\nðŸŽ¯ Training completed. Best validation micro-F1 = {best_val_microF1:.4f}\")\n",
        "    return model, history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Zpx6C939I5xD",
      "metadata": {
        "id": "Zpx6C939I5xD"
      },
      "outputs": [],
      "source": [
        "model, history = train(\n",
        "    model=model,\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    device=device\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gw7k3d1FKLpi",
      "metadata": {
        "id": "gw7k3d1FKLpi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
